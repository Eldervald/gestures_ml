{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401a732a-f87b-47da-9785-4e22f97c2762",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.25 s, sys: 1.27 s, total: 6.53 s\n",
      "Wall time: 17.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from arcface import ArcFaceLoss\n",
    "import graphviz\n",
    "from torchmetrics.classification import MulticlassAUROC, MulticlassAccuracy, MulticlassConfusionMatrix\n",
    "import torchvision\n",
    "import json\n",
    "import pandas as pd\n",
    "from torchmetrics import MetricCollection\n",
    "import os\n",
    "import seaborn as sns\n",
    "import torchvision.transforms.v2 as tf\n",
    "from torchvision.models.video.swin_transformer import swin3d_b, Swin3D_B_Weights\n",
    "from torchvision.models.video.swin_transformer import swin3d_t, Swin3D_T_Weights\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from lsfb_dataset.utils.download.dataset_downloader import DatasetDownloader\n",
    "from torchvision.datasets import DatasetFolder\n",
    "from torchvision.io import read_video\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "218ccefc-2d5c-4227-a7c5-ad88ccaf8c69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# destination_folder = '../data/LSFB'\n",
    "\n",
    "# ds = DatasetDownloader(destination_folder,\n",
    "#                        dataset=\"isol\",\n",
    "#                        include_video=True,\n",
    "#                        compute_hash=True,\n",
    "#                        landmarks=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a38a4a23-2451-42e5-b02a-bd9895a86936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ds.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bb1a713-22d6-44d4-94d1-3ddd8bbd84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = DatasetFolder('../data/LSFB/videos/', loader=lambda x: read_video(x, pts_unit='sec')[0], extensions='mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d7d76dc-c4b9-4e9e-bc58-7f33b90c9d74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FRAMES_PER_VIDEO = 8\n",
    "RESIZE_SHAPE = 256\n",
    "EMB_SIZE = 128\n",
    "SEED = 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23356433-f202-427f-9ab4-03480fb1e47e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform = A.Compose([\n",
    "     A.LongestMaxSize(max_size=RESIZE_SHAPE),\n",
    "])\n",
    "\n",
    "def uniform_temporal_subsample(data, n_samples=FRAMES_PER_VIDEO):\n",
    "    n_frames = len(data)\n",
    "    indices = np.round(np.linspace(0, n_frames - 1, n_samples)).astype(np.int32)\n",
    "    data = data[indices]\n",
    "    return data\n",
    "\n",
    "def apply_transform(video):\n",
    "    video = uniform_temporal_subsample(video)\n",
    "    res = []\n",
    "    for frame in video:\n",
    "        res.append(transform(image=frame)['image'])\n",
    "    return np.array(res)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9cce5d6-3e03-4589-9fdb-3a8939f9edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_np_ds(dir_path='../data/LSFB/np8'):\n",
    "    ds_path = dir_path\n",
    "    Path(ds_path).mkdir(parents=True, exist_ok=True)\n",
    "    for idx in tqdm(range(len(ds))):\n",
    "        sample = ds[idx]\n",
    "        vid, cls = sample\n",
    "        vid = apply_transform(vid.numpy())\n",
    "        cls_dir_path = os.path.join(ds_path, str(cls))\n",
    "        Path(cls_dir_path).mkdir(parents=True, exist_ok=True)\n",
    "        np.save(os.path.join(cls_dir_path, str(idx)), vid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87c22ae6-70d4-4bef-978f-509211413c21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create_np_ds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "659e8c57-15db-4209-9c12-d144b1638f23",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset DatasetFolder\n",
       "    Number of datapoints: 96401\n",
       "    Root location: ../data/LSFB/np8/\n",
       "    StandardTransform\n",
       "Transform: NpToTensor()"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_INPUT_SIZE = 200\n",
    "\n",
    "class NpToTensor(torch.nn.Module):\n",
    "    @staticmethod\n",
    "    def forward(img_list):\n",
    "        return (torch.tensor(img_list.transpose(0, 3, 1, 2)).float() / 255)\n",
    "    \n",
    "ds_path = '../data/LSFB/np8/'\n",
    "ds = torchvision.datasets.DatasetFolder(\n",
    "                    root=ds_path,\n",
    "                    loader=np.load,\n",
    "                    transform= NpToTensor(),\n",
    "                    extensions=('.npy'), )\n",
    "# VideoFrameDataset(\n",
    "#     'data/autsl_frames/train',\n",
    "#     'data/autsl_frames/train/annotations.txt',\n",
    "#     8,\n",
    "#     1,\n",
    "#     imagefile_template= 'img_{:04d}.jpg',\n",
    "#     transform=frame_transforms['train']\n",
    "# )\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a422d837-020b-4808-9f97-8da60ccfdb6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(ds))\n",
    "test_size = len(ds) - train_size\n",
    "train_ds, test_ds = torch.utils.data.random_split(ds, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d03ae993-1433-4467-8097-e3ec93d7e6fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(ds, batch_size=24, shuffle=True, num_workers=8, drop_last=True, pin_memory=False)#, collate_fn=batch_as_list_collate)\n",
    "# val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=8, pin_memory=False)#, collate_fn=batch_as_list_collate)\n",
    "test_loader = DataLoader(test_ds, batch_size=24, shuffle=False, num_workers=8, pin_memory=False)#, collate_fn=batch_as_list_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6edcfa2f-a533-4df7-81f6-24fe411d7763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(ds.classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a471818f-54ea-4d1b-ae75-ec2572c17778",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SwinTransformer(nn.Module):\n",
    "    def __init__(self, out_dim):\n",
    "        super().__init__()\n",
    "        self.model = swin3d_t(weights=Swin3D_T_Weights.DEFAULT)\n",
    "        self.model.head = nn.Linear(in_features=768, out_features=out_dim, bias=True)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return self.model(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86d3e854-29af-4d39-a9f0-9feb31646152",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LITModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        emb_dim: float,\n",
    "        drop_rate: float,\n",
    "        eta_min: float,\n",
    "        learning_rate: float,\n",
    "        loss: str,\n",
    "        num_classes: int,\n",
    "        optimizer: str,\n",
    "        scheduler: str,\n",
    "        weight_decay: float,\n",
    "        pretrained_path: str = None,\n",
    "        pretrained_frame_encoder_path: str = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.model = self._init_model(pretrained_path, pretrained_frame_encoder_path)\n",
    "\n",
    "        self.loss_fn = self._init_loss_fn()\n",
    "\n",
    "        self.metrics = self._init_metrics()\n",
    "        \n",
    "        self.transforms = self._init_transforms()\n",
    "\n",
    "    def _init_model(self, pretrained_path=None, pretrained_frame_encoder_path=None):\n",
    "        if pretrained_path is not None:\n",
    "            return torch.load(pretrained_path)\n",
    "        \n",
    "        # model = VideoClassifier(\n",
    "        #         emb_dim=self.hparams.emb_dim,\n",
    "        #         num_classes=self.hparams.num_classes,\n",
    "        #     )\n",
    "        \n",
    "        model = SwinTransformer(\n",
    "            out_dim=EMB_SIZE)\n",
    "        \n",
    "        if pretrained_frame_encoder_path is not None:\n",
    "            model.frame_enc = torch.load(pretrained_frame_encoder_path)\n",
    "            \n",
    "        return model\n",
    "            \n",
    "\n",
    "    def _init_loss_fn(self):\n",
    "        if self.hparams.loss == \"Arcface\":\n",
    "            loss = ArcFaceLoss(in_features=EMB_SIZE, num_classes=self.hparams.num_classes)\n",
    "            if self.model.model.head.in_features != EMB_SIZE:\n",
    "                self.model.model.head = nn.Linear(768, EMB_SIZE)\n",
    "            return loss\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"{self.hparams.loss} is not a valid loss function\")\n",
    "\n",
    "    def _init_metrics(self):\n",
    "        metrics = {\n",
    "                   # 'rocauc': MulticlassAUROC(self.hparams.num_classes),\n",
    "                   'acc@1' : MulticlassAccuracy(self.hparams.num_classes, top_k=1),\n",
    "                   'acc@5' : MulticlassAccuracy(self.hparams.num_classes, top_k=5),\n",
    "                   'acc@10': MulticlassAccuracy(self.hparams.num_classes, top_k=10),\n",
    "        }\n",
    "        metric_collection = MetricCollection(metrics)\n",
    "\n",
    "        return torch.nn.ModuleDict(\n",
    "            {\n",
    "                \"train_metrics\": metric_collection.clone(prefix=\"train_\"),\n",
    "                \"val_metrics\": metric_collection.clone(prefix=\"val_\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self._init_optimizer()\n",
    "\n",
    "        scheduler = self._init_scheduler(optimizer)\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def _init_optimizer(self):\n",
    "        if self.hparams.optimizer == 'AdamW':\n",
    "            optimizer = torch.optim.AdamW(\n",
    "                params=self.parameters(),\n",
    "                lr=self.hparams.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown optimizer: {self.hparams.optimizer}\")\n",
    "        return optimizer\n",
    "\n",
    "    def _init_scheduler(self, optimizer):\n",
    "        if self.hparams.scheduler == \"CosineAnnealingLR\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer,\n",
    "                T_max=self.trainer.max_epochs,\n",
    "                eta_min=self.hparams.eta_min,\n",
    "            )\n",
    "        elif self.hparams.scheduler == \"StepLR\":\n",
    "            scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "                optimizer,\n",
    "                step_size=self.trainer.max_epochs // 5,\n",
    "                gamma=0.95,\n",
    "            )\n",
    "        elif self.hparams.scheduler == \"OneCycleLR\":\n",
    "            scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=self.hparams.learning_rate,\n",
    "                total_steps=self.trainer.estimated_stepping_batches,\n",
    "                pct_start=2.5 / self.trainer.max_epochs,\n",
    "                final_div_factor=50,\n",
    "                div_factor=100\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown scheduler: {self.hparams.scheduler}\")\n",
    "        return scheduler\n",
    "    \n",
    "    def _init_transforms(self):\n",
    "        swin_transform = Swin3D_T_Weights.DEFAULT.transforms()\n",
    "        transforms = {\n",
    "            'train': nn.Sequential(\n",
    "                tf.RandomApply([tf.ElasticTransform()], p=0.5),\n",
    "                tf.RandomPhotometricDistort(p=0.9),\n",
    "                tf.RandomPerspective(distortion_scale=0.35, p=0.7),\n",
    "                swin_transform),\n",
    "            'val': swin_transform\n",
    "        }\n",
    "        \n",
    "        # transforms = {\n",
    "        #     'train': transform,\n",
    "        #     'val': transform\n",
    "        # }\n",
    "            \n",
    "        return transforms\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch):\n",
    "        return self._shared_step(batch, \"train\")\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        self._log_on_epoch_end('train')\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        self._shared_step(batch, \"val\")\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self._log_on_epoch_end('val')\n",
    "\n",
    "    # TODO\n",
    "    def predict_step(self, batch, batch_idx):\n",
    "        _, labels, logits = self._forward_pass(batch, stage=None)\n",
    "        preds = F.softmax(logits, dim=-1)\n",
    "        return preds, labels\n",
    "\n",
    "    def _shared_step(self, batch, stage):\n",
    "        x, y, emb = self._forward_pass(batch, stage)\n",
    "\n",
    "        loss, y_pred = self.loss_fn(emb, y)\n",
    "\n",
    "        self.metrics[f\"{stage}_metrics\"](y_pred, y)\n",
    "\n",
    "        self._log_on_step(stage, loss, batch_size=len(x))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _forward_pass(self, batch, stage):\n",
    "        x, y = batch\n",
    "        y = y.view(-1)\n",
    "        # batch_size, seq_len = x.shape[:2]\n",
    "        # x = self.transforms[stage](x.flatten(0, 1))\n",
    "        # x = x.reshape(batch_size, seq_len, *x.shape[1:])\n",
    "        x = self.transforms[stage](x)\n",
    "        emb = self(x)\n",
    "\n",
    "        return x, y, emb\n",
    "\n",
    "    def _log_on_step(self, stage, loss, batch_size):\n",
    "        self.log(f\"{stage}_loss\", loss, batch_size=batch_size)\n",
    "\n",
    "    def _log_on_epoch_end(self, stage):\n",
    "        self.log_dict(self.metrics[f\"{stage}_metrics\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dafe44d0-df62-4583-a386-9b251987558b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "module = LITModule(\n",
    "    drop_rate = 0.4,\n",
    "    eta_min = 1e-6,\n",
    "    learning_rate= 1e-4,\n",
    "    loss = \"Arcface\",\n",
    "    num_classes=NUM_CLASSES,\n",
    "    emb_dim = EMB_SIZE,\n",
    "    optimizer = \"AdamW\",\n",
    "    scheduler = \"OneCycleLR\",\n",
    "    weight_decay= 1e-6,\n",
    "    pretrained_path='swint_arcface.ckpt',\n",
    "    # pretrained_frame_encoder_path='frame_enc.ckpt',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71fc835c-ac31-46e5-9da0-6b7a67d69d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meldervald\u001b[0m (\u001b[33mvaloebi\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.14.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20230405_141652-uhmup09p</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/valoebi/gestures/runs/uhmup09p' target=\"_blank\">bajoran-daedalus-145</a></strong> to <a href='https://wandb.ai/valoebi/gestures' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/valoebi/gestures' target=\"_blank\">https://wandb.ai/valoebi/gestures</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/valoebi/gestures/runs/uhmup09p' target=\"_blank\">https://wandb.ai/valoebi/gestures/runs/uhmup09p</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "WANDB_APIKEY = 'a5bd0bef55f8b72f59ac12d24f1623ad19eeb67b'\n",
    "wandb.login(key=WANDB_APIKEY)\n",
    "logger = WandbLogger(project='gestures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f093e9-39a3-4105-b594-4afcea9ede97",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a09fb2a-06e0-4285-82be-e706e9689764",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    }
   ],
   "source": [
    "# logger = TensorBoardLogger(\"../tf_logs/\", name=f\"5mf-img_embs-{cnn_model_name}-{EMB_SIZE}-{num_classes}cls+gradcam\")\n",
    "lr_monitor = LearningRateMonitor(logging_interval='step')\n",
    "from pytorch_lightning.profilers import AdvancedProfiler, SimpleProfiler\n",
    "# profiler = AdvancedProfiler(dirpath='./', filename='adv_profiler_logs')\n",
    "trainer = pl.Trainer(accelerator='gpu',\n",
    "                     devices=1,\n",
    "                     logger=logger,\n",
    "                     accumulate_grad_batches=8,\n",
    "                     # auto_lr_find=True,\n",
    "                     log_every_n_steps=1,\n",
    "                     max_epochs=30,\n",
    "                     # max_steps=60,\n",
    "                     val_check_interval=1.0,\n",
    "                     callbacks=[lr_monitor],\n",
    "                     profiler=SimpleProfiler(dirpath='./', filename='profiler_logs'),\n",
    "                     # profiler=profiler,\n",
    "                     # track_grad_norm=2,\n",
    "                     # gradient_clip_val=10,\n",
    "                     # gradient_clip_algorithm=\"norm\",\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eabf8091-9792-415c-b4b6-e5045fcd9ffa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainer.tune(model, train_loader, val_loader, lr_find_kwargs={\"num_training\":200})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d433d048-d0a0-4333-bfb3-48377a8198e4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "\n",
      "  | Name    | Type            | Params\n",
      "--------------------------------------------\n",
      "0 | model   | SwinTransformer | 27.9 M\n",
      "1 | loss_fn | ArcFaceLoss     | 534 K \n",
      "2 | metrics | ModuleDict      | 0     \n",
      "--------------------------------------------\n",
      "28.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.5 M    Total params\n",
      "113.933   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815bc0e4ed43492680ac9de25c141402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(module, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cf9e3864-1c40-4b24-b5dd-11cefe9334e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(module.model, f'swint_arcface.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "977f3f27-9468-4ee6-900f-90c236ad9370",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "cnt = Counter([c for p,c in ds.samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "78c8db82-8475-41b8-b203-cfbae2a37f61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5.179934e+06, 1.974302e+06, 1.285592e+06, ..., 0.000000e+00,\n",
       "        0.000000e+00, 4.174000e+03]),\n",
       " array([1.00000000e+00, 1.99962547e+00, 2.99925094e+00, ...,\n",
       "        2.66800075e+03, 2.66900037e+03, 2.67000000e+03]),\n",
       " <BarContainer object of 2670 artists>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist([v for k,v in cnt.items()for k,v in cnt.items()], bins=max([v for k,v in cnt.items()for k,v in cnt.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b465b59a-ab78-42e2-b4cd-39996d9a6578",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3: 308,\n",
       "         1: 1241,\n",
       "         10: 83,\n",
       "         42: 8,\n",
       "         55: 11,\n",
       "         6: 127,\n",
       "         26: 18,\n",
       "         25: 30,\n",
       "         2: 473,\n",
       "         31: 17,\n",
       "         14: 49,\n",
       "         4: 222,\n",
       "         17: 39,\n",
       "         16: 32,\n",
       "         5: 164,\n",
       "         12: 62,\n",
       "         35: 14,\n",
       "         15: 49,\n",
       "         11: 61,\n",
       "         54: 3,\n",
       "         7: 130,\n",
       "         8: 101,\n",
       "         9: 94,\n",
       "         27: 18,\n",
       "         13: 61,\n",
       "         254: 1,\n",
       "         664: 1,\n",
       "         43: 10,\n",
       "         121: 2,\n",
       "         80: 3,\n",
       "         33: 13,\n",
       "         72: 4,\n",
       "         29: 18,\n",
       "         21: 30,\n",
       "         62: 7,\n",
       "         742: 1,\n",
       "         30: 17,\n",
       "         64: 3,\n",
       "         103: 2,\n",
       "         34: 16,\n",
       "         86: 4,\n",
       "         45: 6,\n",
       "         430: 1,\n",
       "         39: 15,\n",
       "         19: 37,\n",
       "         68: 2,\n",
       "         105: 4,\n",
       "         142: 2,\n",
       "         324: 1,\n",
       "         226: 1,\n",
       "         22: 31,\n",
       "         38: 7,\n",
       "         238: 2,\n",
       "         46: 13,\n",
       "         181: 3,\n",
       "         18: 29,\n",
       "         97: 2,\n",
       "         23: 27,\n",
       "         305: 2,\n",
       "         70: 2,\n",
       "         63: 4,\n",
       "         760: 1,\n",
       "         50: 6,\n",
       "         36: 9,\n",
       "         136: 3,\n",
       "         322: 1,\n",
       "         258: 2,\n",
       "         20: 25,\n",
       "         340: 1,\n",
       "         66: 7,\n",
       "         24: 22,\n",
       "         49: 6,\n",
       "         419: 1,\n",
       "         112: 2,\n",
       "         117: 2,\n",
       "         127: 3,\n",
       "         79: 4,\n",
       "         59: 8,\n",
       "         120: 2,\n",
       "         801: 1,\n",
       "         75: 4,\n",
       "         107: 2,\n",
       "         61: 4,\n",
       "         28: 16,\n",
       "         176: 2,\n",
       "         47: 4,\n",
       "         32: 13,\n",
       "         58: 7,\n",
       "         536: 1,\n",
       "         116: 2,\n",
       "         60: 4,\n",
       "         390: 2,\n",
       "         83: 3,\n",
       "         51: 6,\n",
       "         102: 4,\n",
       "         98: 3,\n",
       "         99: 3,\n",
       "         161: 1,\n",
       "         145: 1,\n",
       "         366: 1,\n",
       "         138: 2,\n",
       "         52: 5,\n",
       "         41: 8,\n",
       "         139: 2,\n",
       "         123: 1,\n",
       "         100: 4,\n",
       "         96: 3,\n",
       "         53: 15,\n",
       "         67: 2,\n",
       "         397: 1,\n",
       "         230: 1,\n",
       "         77: 6,\n",
       "         37: 14,\n",
       "         163: 2,\n",
       "         65: 2,\n",
       "         124: 2,\n",
       "         114: 2,\n",
       "         57: 9,\n",
       "         93: 1,\n",
       "         40: 9,\n",
       "         44: 5,\n",
       "         209: 1,\n",
       "         111: 2,\n",
       "         88: 2,\n",
       "         222: 2,\n",
       "         306: 1,\n",
       "         140: 2,\n",
       "         203: 1,\n",
       "         357: 1,\n",
       "         113: 2,\n",
       "         188: 3,\n",
       "         173: 1,\n",
       "         48: 6,\n",
       "         2471: 1,\n",
       "         998: 1,\n",
       "         56: 3,\n",
       "         232: 1,\n",
       "         183: 1,\n",
       "         78: 4,\n",
       "         92: 2,\n",
       "         115: 1,\n",
       "         119: 2,\n",
       "         319: 1,\n",
       "         320: 2,\n",
       "         76: 3,\n",
       "         91: 2,\n",
       "         134: 1,\n",
       "         192: 2,\n",
       "         196: 2,\n",
       "         948: 2,\n",
       "         177: 1,\n",
       "         71: 2,\n",
       "         439: 1,\n",
       "         84: 1,\n",
       "         165: 1,\n",
       "         160: 2,\n",
       "         174: 1,\n",
       "         285: 1,\n",
       "         164: 1,\n",
       "         1898: 1,\n",
       "         90: 1,\n",
       "         251: 1,\n",
       "         420: 1,\n",
       "         249: 1,\n",
       "         315: 1,\n",
       "         141: 1,\n",
       "         159: 2,\n",
       "         187: 1,\n",
       "         228: 2,\n",
       "         448: 1,\n",
       "         444: 1,\n",
       "         198: 1,\n",
       "         125: 1,\n",
       "         69: 3,\n",
       "         883: 1,\n",
       "         298: 1,\n",
       "         384: 1,\n",
       "         427: 1,\n",
       "         463: 1,\n",
       "         241: 2,\n",
       "         146: 2,\n",
       "         205: 3,\n",
       "         2670: 1,\n",
       "         110: 2,\n",
       "         74: 1,\n",
       "         1108: 1,\n",
       "         81: 1,\n",
       "         166: 2,\n",
       "         130: 1,\n",
       "         122: 1,\n",
       "         175: 1,\n",
       "         814: 1,\n",
       "         356: 1,\n",
       "         753: 1,\n",
       "         82: 1,\n",
       "         200: 1,\n",
       "         219: 1,\n",
       "         488: 1,\n",
       "         1257: 1,\n",
       "         150: 1,\n",
       "         1260: 1,\n",
       "         73: 3,\n",
       "         464: 1,\n",
       "         148: 1,\n",
       "         225: 1,\n",
       "         128: 2,\n",
       "         85: 2,\n",
       "         162: 1,\n",
       "         533: 1,\n",
       "         94: 1,\n",
       "         470: 1,\n",
       "         195: 1,\n",
       "         204: 1,\n",
       "         665: 1,\n",
       "         158: 1,\n",
       "         89: 2,\n",
       "         236: 1,\n",
       "         580: 1,\n",
       "         87: 2,\n",
       "         826: 1,\n",
       "         101: 1,\n",
       "         391: 1,\n",
       "         186: 1,\n",
       "         449: 1,\n",
       "         437: 1,\n",
       "         109: 1,\n",
       "         118: 1,\n",
       "         511: 1})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter([v for k,v in cnt.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0861f020-d47c-492e-b2c7-98f9d1be7e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2152"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([k for k,v in cnt.items() if v >= 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34aff4e4-91cb-44da-8ca2-37e6939b5638",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2933"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96615471-0391-44fb-809f-bed24b9e294b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
