{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a92b4306-d5c0-4be5-9785-9ad622fc057a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.06 s, sys: 1.03 s, total: 6.09 s\n",
      "Wall time: 5.14 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/datapoints/__init__.py:12: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/transforms/v2/__init__.py:54: UserWarning: The torchvision.datapoints and torchvision.transforms.v2 namespaces are still Beta. While we do not expect major breaking changes, some APIs may still change according to user feedback. Please submit any feedback you may have in this issue: https://github.com/pytorch/vision/issues/6753, and you can also check out https://github.com/pytorch/vision/issues/7319 to learn more about the APIs that we suspect might involve future changes. You can silence this warning by calling torchvision.disable_beta_transforms_warning().\n",
      "  warnings.warn(_BETA_TRANSFORMS_WARNING)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.datasets import ImageFolder\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader, random_split, Dataset\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from src.arcface import ArcFaceLoss\n",
    "import graphviz\n",
    "from torchmetrics.classification import MulticlassAUROC, MulticlassAccuracy, MulticlassConfusionMatrix\n",
    "import torchvision\n",
    "import json\n",
    "import pandas as pd\n",
    "from torchmetrics import MetricCollection\n",
    "import os\n",
    "import seaborn as sns\n",
    "import torchvision.transforms.v2 as tf\n",
    "from src.ds import VideoFrameDataset, ImglistToTensor\n",
    "from src.video_utils import read_video\n",
    "from torchvision.models.video.swin_transformer import swin3d_b, Swin3D_B_Weights\n",
    "from torchvision.models.video.swin_transformer import swin3d_t, Swin3D_T_Weights\n",
    "\n",
    "\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "torch.set_float32_matmul_precision(\"medium\")\n",
    "\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image\n",
    "from pytorch_grad_cam import GradCAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b09c614e-f2f3-494c-a370-2b309ed1adca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class RSL_DS(torch.utils.data.Dataset):\n",
    "    def __init__(self, annotations_path='data/rsl/annotations.csv', ds_type='train'):\n",
    "        super().__init__()\n",
    "        self.df = pd.read_csv(annotations_path, sep='\\t')\n",
    "        self.df = self.df[self.df['text'] != 'no_event']\n",
    "        self.ds_type = ds_type\n",
    "\n",
    "        if self.ds_type == 'train':\n",
    "            self.df = self.df[self.df['train']]\n",
    "        elif self.ds_type == 'test':\n",
    "            self.df = self.df[~self.df['train']]\n",
    "        else:\n",
    "            raise Exception(\"Invalid ds type\")\n",
    "\n",
    "        self.classes = list(self.df['text'].unique())\n",
    "        self.text_to_id = {text: i for i, text in enumerate(self.classes)}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.df.iloc[idx]\n",
    "        cls = self.text_to_id[sample['text']]\n",
    "\n",
    "        vid, _ = read_video(os.path.join(f'data/rsl/{self.ds_type}', sample['attachment_id'] + '.mp4'))\n",
    "        \n",
    "        return vid.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "565d4ddd-33ca-4926-a3bc-7756748871d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = RSL_DS()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a59cb6e-ec60-4f6f-ab81-fa9d201caa07",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1695d56abe354f1a8208e4ce81a4f003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "with Pool() as pool:\n",
    "    ll = []\n",
    "    for l in tqdm(pool.imap(lambda i: ds[np.random.randint(len(ds))], range(1000)), total=1000):\n",
    "        ll.append(l)\n",
    "    pool.close()\n",
    "    pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0bf02a96-8f46-461d-8353-dfbf66783bf8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234.375"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*20000 * 256*256 * 3 * 4 / 1024/1024/1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea41e5c-355e-4d27-b5d1-d50c196bc8db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
