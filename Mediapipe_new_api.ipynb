{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c7a7ab15-8ad5-40d8-9a8c-8f2a1d580e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2 \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "from multiprocessing.pool import ThreadPool as Pool\n",
    "from gc import collect\n",
    "from mediapipe.python.solutions.drawing_utils import DrawingSpec\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from src.video_utils import read_video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "f353be13-09eb-43f2-aaba-1996061a8848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 512, 512, 3)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video, timestamps = read_video('../../Datasets/AUTSL/test/signer6_sample1_color.mp4')\n",
    "video.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "87e8bd3f-313f-471b-88d1-513c1d2c3118",
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "PoseLandmarker = mp.tasks.vision.PoseLandmarker\n",
    "PoseLandmarkerOptions = mp.tasks.vision.PoseLandmarkerOptions\n",
    "\n",
    "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6b27491d-9566-4518-8003-5bc5c84f10f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pose landmarker instance with the video mode:\n",
    "POSE_OPTIONS = PoseLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='mediapipe_models/pose_landmarker_heavy.task'),\n",
    "    running_mode=VisionRunningMode.VIDEO)\n",
    "\n",
    "# Create a hand landmarker instance with the video mode:\n",
    "HAND_OPTIONS = HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='mediapipe_models/hand_landmarker.task'),\n",
    "    running_mode=VisionRunningMode.VIDEO,\n",
    "    num_hands=10,\n",
    "    min_hand_detection_confidence=0.3,\n",
    "    min_hand_presence_confidence=0.3,\n",
    ")\n",
    "\n",
    "POSE_HAND_BASIS = {'Left':  np.array([15, 17, 19, 21]),\n",
    "                    'Right': np.array([16, 18, 20, 22])}\n",
    "HAND_BASIS = np.array([0, 17, 5, 2])\n",
    "\n",
    "def get_skeleton_edges():\n",
    "    del_mask = (np.arange(75) >= 15) & (np.arange(75) <= 22)  # True if point deleted\n",
    "    pose_edges = mp.solutions.pose.POSE_CONNECTIONS\n",
    "    hand_edges = mp.solutions.hands.HAND_CONNECTIONS\n",
    "    edges = []\n",
    "    for (u, v) in pose_edges:\n",
    "        if del_mask[u] or del_mask[v]:\n",
    "            continue\n",
    "        edges.append((u, v))\n",
    "    for (u, v) in hand_edges:\n",
    "        edges.append((u + 33, v + 33))\n",
    "        edges.append((u + 54, v + 54))\n",
    "    edges.append((13, 33))\n",
    "    edges.append((14, 54))\n",
    "    return tuple(edges)\n",
    "\n",
    "\n",
    "SKELETON_EDGES = get_skeleton_edges()\n",
    "\n",
    "FRAME_SHAPE = np.array(video.shape[1:3])\n",
    "\n",
    "np.random.seed(2312)\n",
    "EDGE_COLORS = tuple(tuple(int(i) for i in np.random.randint(10, 246, 3))\n",
    "                    for e in SKELETON_EDGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "6858ad97-c186-4df9-83d3-ef32d201b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_pose(video, timestamps):\n",
    "    cnt_frames = len(video)\n",
    "    with PoseLandmarker.create_from_options(POSE_OPTIONS) as pose_model:\n",
    "        res = []\n",
    "        for frame, timestamp in zip(video, timestamps):\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            res.append(pose_model.detect_for_video(mp_image, timestamp))\n",
    "\n",
    "        pose_points = np.empty((cnt_frames, 33, 2)) + np.nan\n",
    "        for i, frame_res in enumerate(res):\n",
    "            if frame_res.pose_landmarks is None:\n",
    "                continue\n",
    "            pose_points[i] = np.array([\n",
    "                [lm.x, lm.y] for lm in frame_res.pose_landmarks[0]\n",
    "            ])\n",
    "    return pose_points\n",
    "\n",
    "\n",
    "def detect_all_hands(video, timestamps):\n",
    "    cnt_frames = len(video)\n",
    "    with HandLandmarker.create_from_options(HAND_OPTIONS) as hand_model:\n",
    "        res = []\n",
    "        for frame, timestamp in zip(video, timestamps):\n",
    "            mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)\n",
    "            res.append(hand_model.detect_for_video(mp_image, timestamp))\n",
    "\n",
    "        hands = [{'Left': [], 'Right':[]}\n",
    "                 for i in range(cnt_frames)]\n",
    "        \n",
    "        for i, frame_res in enumerate(res):\n",
    "            if not len(frame_res.handedness):\n",
    "                continue\n",
    "            for hand_index, hand_info in enumerate(frame_res.handedness):\n",
    "                hands[i][hand_info[0].category_name].append(\n",
    "                    np.array([\n",
    "                        [lm.x, lm.y]\n",
    "                        for lm in frame_res.hand_landmarks[hand_index]\n",
    "                    ])\n",
    "                )\n",
    "            \n",
    "            # unmirror image\n",
    "            hands[i]['Left'], hands[i]['Right'] = hands[i]['Right'], hands[i]['Left']\n",
    "            \n",
    "    return hands\n",
    "\n",
    "\n",
    "def match_distance(pose_hand_points, hand_points):\n",
    "    return ((pose_hand_points - hand_points)**2).sum(axis=-1).mean()\n",
    "\n",
    "\n",
    "def match_hands(pose_points, all_hands_points):\n",
    "    hand_points_shape = (pose_points.shape[0], 21, 2)\n",
    "    hand_points = {\n",
    "        'Left': np.empty(hand_points_shape) + np.nan,\n",
    "        'Right': np.empty(hand_points_shape) + np.nan\n",
    "    }\n",
    "    \n",
    "\n",
    "    for i, (pose, hand_group) in enumerate(zip(pose_points, all_hands_points)):\n",
    "        for side in hand_points:\n",
    "            if len(hand_group[side]) > 0:\n",
    "                d = np.array([\n",
    "                    match_distance(pose[POSE_HAND_BASIS[side]], hand[HAND_BASIS])\n",
    "                    for hand in hand_group[side]\n",
    "                ])\n",
    "\n",
    "                # if all hand are on large distance, leave nan?\n",
    "                # print(i, np.round(d.min(), decimals=7))\n",
    "                if d.min() > 0.01:\n",
    "                    continue\n",
    "                # ----------------------------------------------\n",
    "\n",
    "                index = d.argmin()\n",
    "                hand_points[side][i] = hand_group[side][index]\n",
    "\n",
    "    return hand_points\n",
    "\n",
    "\n",
    "def detect_skeleton_points(video, timestamps):\n",
    "    # If video frames have different width and heights, video is padded by zeros\n",
    "    if video.shape[1] != video.shape[2]:\n",
    "        longest_side = max(video.shape[1:3])\n",
    "        new_shape = (video.shape[0], longest_side, longest_side, *video.shape[3:])\n",
    "        new_video = np.zeros(new_shape, dtype=video.dtype)\n",
    "        pos = tuple(((new_side - old_side) // 2, (new_side - old_side) // 2 + old_side)\n",
    "                    for old_side, new_side in zip(video.shape[1:3], new_shape[1:3]))\n",
    "        new_video[:, pos[0][0]: pos[0][1], pos[1][0]: pos[1][1]] = video\n",
    "        video = new_video\n",
    "        \n",
    "    pose_points = detect_pose(video, timestamps)\n",
    "    all_hands_points = detect_all_hands(video, timestamps)\n",
    "    hands_points = match_hands(pose_points, all_hands_points)\n",
    "\n",
    "    skeleton_points = np.concatenate(\n",
    "        (pose_points, hands_points['Left'], hands_points['Right']),\n",
    "        axis=1\n",
    "    )\n",
    "    return skeleton_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a0d91b4d-d8b9-49d6-bde7-78c6c9e76269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_skeleton(skeleton):\n",
    "    frame = np.zeros((*FRAME_SHAPE, 3), dtype=np.uint8)\n",
    "    for (u, v), color in zip(SKELETON_EDGES, EDGE_COLORS):\n",
    "        if np.isnan(skeleton[u]).any() or np.isnan(skeleton[v]).any(): \n",
    "            continue\n",
    "        \n",
    "        frame = cv2.line(frame,\n",
    "                         np.round(skeleton[u] * FRAME_SHAPE).astype(np.int32),\n",
    "                         np.round(skeleton[v] * FRAME_SHAPE).astype(np.int32),\n",
    "                         color,\n",
    "                         1)\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "66126ddd-e64e-4b49-9564-0739d0e357bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "skel = detect_skeleton_points(video[:, :, 100:-100], timestamps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9165b200-52ed-4e22-b457-393166c6f90b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for frame, sk in zip(video, skel):\n",
    "    plt.figure(figsize=(20,20))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(frame)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(draw_skeleton(sk))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "ffef4f74-a372-4363-9cda-06d6184f24c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_path(video_path):\n",
    "    return video_path, detect_skeleton_points(*read_video(video_path))\n",
    "\n",
    "def extract_skeletons_for_videos(path_to_videos):\n",
    "    skeleton_mapper = {}\n",
    "    try:\n",
    "        all_video_paths = (os.path.join(path_to_videos, video_name) for video_name in os.listdir(path_to_videos))\n",
    "        with Pool() as pool:\n",
    "            for video_path, skeleton in tqdm(pool.imap(process_path, all_video_paths), total=len(os.listdir(path_to_videos))):\n",
    "                skeleton_mapper[video_path] = skeleton\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "    \n",
    "    return skeleton_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "ff4cb6a3-a897-4e6e-92fe-58cda3d2bde1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1aef20b1add48b5beed8201fe704fc0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "path_to_videos = '../../Datasets/AUTSL/test/'\n",
    "res = extract_skeletons_for_videos(path_to_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "29123470-79d9-4016-a95f-91f514a5b192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1569/3105621980.py:9: RuntimeWarning: invalid value encountered in add\n",
      "  pose_points = np.empty((cnt_frames, 33, 2)) + np.nan\n"
     ]
    }
   ],
   "source": [
    "len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "40afec97-b8c1-42b8-8364-a0a03242ec52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55, 75, 2)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf66b197-104c-418f-8b9f-c408e4e84fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
